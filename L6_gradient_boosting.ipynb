{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNleEyKUJFj1oBSHDmw9D/h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayamlearning/ML_Zoom_Camp/blob/main/L6_gradient_boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhKRxe0ZT28E"
      },
      "outputs": [],
      "source": [
        "#installation\n",
        "!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "RhslCoLZT8K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"housing.csv\")\n",
        "data.sample(5)"
      ],
      "metadata": {
        "id": "b__kUud3UFFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing the dataset"
      ],
      "metadata": {
        "id": "vrLWxbRmXyPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_log_transform(y):\n",
        "  return np.log1p(y)\n",
        "\n",
        "apply_log_transform(2000)"
      ],
      "metadata": {
        "id": "TvlLAHQAYYaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = data.copy()"
      ],
      "metadata": {
        "id": "JQRSaTUpalmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#First, keep only the records where ocean_proximity is either '<1H OCEAN' or 'INLAND'\n",
        "df = df.loc[(df['ocean_proximity']=='<1H OCEAN') | (df['ocean_proximity']=='INLAND')]"
      ],
      "metadata": {
        "id": "AUbDHAz7UQvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fill missing values with zeros\n",
        "df = df.fillna(0)\n",
        "\n",
        "#Apply the log transform to median_house_value\n",
        "df.median_house_value = df['median_house_value'].apply(apply_log_transform)"
      ],
      "metadata": {
        "id": "f_TQhVZJXG4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Do train/validation/test split with 60%/20%/20% distribution.\n",
        "#Use the train_test_split function and set the random_state parameter to 1.\n",
        "\n",
        "df_train_full, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
        "df_train, df_val = train_test_split(df_train_full, test_size=0.25, random_state=1)\n",
        "\n",
        "df_train = df_train.reset_index(drop=True)\n",
        "df_val = df_val.reset_index(drop=True)\n",
        "df_test = df_test.reset_index(drop=True)\n",
        "\n",
        "\n",
        "y_train = df_train.median_house_value.values\n",
        "y_val = df_val.median_house_value.values\n",
        "y_test = df_test.median_house_value.values\n",
        "\n",
        "\n",
        "del df_train['median_house_value']\n",
        "del df_val['median_house_value']\n",
        "del df_test['median_house_value']\n",
        "\n",
        "\n",
        "print(df_train.shape[0], df_val.shape[0], df_test.shape[0] )"
      ],
      "metadata": {
        "id": "bnwt8Bf8aQ6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use DictVectorizer(sparse=True) to turn the dataframes into matrices.\n",
        "train_dict = df_train.to_dict(orient='records')\n",
        "\n",
        "dv = DictVectorizer(sparse=False)\n",
        "X_train = dv.fit_transform(train_dict)\n",
        "\n",
        "val_dict = df_val.to_dict(orient='records')\n",
        "X_val = dv.transform(val_dict)"
      ],
      "metadata": {
        "id": "aoxerVs8aS9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dv.get_feature_names_out()"
      ],
      "metadata": {
        "id": "J9gOFG6Wbx9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluate model"
      ],
      "metadata": {
        "id": "TuKZMiWOckc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's train a decision tree regressor to predict the median_house_value variable.\n",
        "#Train a model with max_depth=1."
      ],
      "metadata": {
        "id": "iClEIezNcmLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor,export_text"
      ],
      "metadata": {
        "id": "vdFfGElOdumR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#max_depth=1\n",
        "dt = DecisionTreeRegressor(max_depth=1)\n",
        "dt.fit(X_train,y_train)\n",
        "\n",
        "y_pred = dt.predict(X_val)\n",
        "rmse = mean_squared_error(y_val,y_pred,squared=False)\n",
        "print(round(rmse,3))"
      ],
      "metadata": {
        "id": "JdlIoJiGsdRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(export_text(dt,feature_names = list(dv.get_feature_names_out())))"
      ],
      "metadata": {
        "id": "tU059TGWfhFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "metadata": {
        "id": "9JbDILc7zGdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Train a random forest model with these parameters:\n",
        "  * n_estimators=10\n",
        "  * random_state=1\n",
        "'''\n",
        "\n",
        "dt = RandomForestRegressor(n_estimators=10, random_state=1)\n",
        "dt.fit(X_train,y_train)\n",
        "\n",
        "y_pred = dt.predict(X_val)\n",
        "rmse = mean_squared_error(y_val,y_pred,squared=False)\n",
        "print(round(rmse,3))\n"
      ],
      "metadata": {
        "id": "lR8Q5pbDs8zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Now let's experiment with the n_estimators parameter\n",
        "  * Try different values of this parameter from 10 to 200 with step 10.\n",
        "  * Set random_state to 1.\n",
        "  * Evaluate the model on the validation dataset.\n",
        "'''\n",
        "\n",
        "lst_estimators_acc = []\n",
        "\n",
        "for i in range(10, 201, 10):\n",
        "  dt = RandomForestRegressor(n_estimators=i,random_state=1)\n",
        "  dt.fit(X_train,y_train)\n",
        "\n",
        "  y_pred = dt.predict(X_val)\n",
        "  rmse = mean_squared_error(y_val,y_pred,squared=False)\n",
        "  lst_estimators_acc.append((i,round(rmse,3)))"
      ],
      "metadata": {
        "id": "v2Dhynf0dwRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_est = pd.DataFrame(lst_estimators_acc, columns = ['estimator','rmse'] )\n",
        "df_est.sort_values(by=['rmse'],ascending=False)\n",
        "df_est.head()"
      ],
      "metadata": {
        "id": "DPXBFhjuoiZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(df_est.estimator,df_est.rmse)"
      ],
      "metadata": {
        "id": "MWmb1l2Tr5Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Let's select the best max_depth:\n",
        "  * Try different values of max_depth: [10, 15, 20, 25]\n",
        "  * For each of these values,\n",
        "  * try different values of n_estimators from 10 till 200 (with step 10)\n",
        "  * calculate the mean RMSE\n",
        "  * Fix the random seed: random_state=1\n",
        "\n",
        "'''\n",
        "lst_estimators_acc = []\n",
        "\n",
        "for e in range(10, 201, 10):\n",
        "  for d in [10, 15, 20, 25]:\n",
        "    dt = RandomForestRegressor(n_estimators=e, max_depth=d, random_state=1)\n",
        "    dt.fit(X_train,y_train)\n",
        "\n",
        "    y_pred = dt.predict(X_val)\n",
        "    rmse = mean_squared_error(y_val,y_pred,squared=False)\n",
        "    lst_estimators_acc.append((e,d,round(rmse,3)))"
      ],
      "metadata": {
        "id": "uVSpH4dA0gk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns =  ['estimator','max_depth','rmse']\n",
        "df_est = pd.DataFrame(lst_estimators_acc, columns =columns).head()\n",
        "df_est.sort_values(by=['rmse'],ascending=False)"
      ],
      "metadata": {
        "id": "FyOFgbgF1Wzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "We can extract feature importance information from tree-based models.\n",
        "At each step of the decision tree learning algorithm, it finds the best split.\n",
        "When doing it, we can calculate \"gain\" - the reduction in impurity before and\n",
        "after the split. This gain is quite useful in understanding what are the\n",
        "important features for tree-based models.\n",
        "\n",
        "In Scikit-Learn, tree-based models contain this information in the feature_importances_ field.\n",
        "\n",
        "For this homework question, we'll find the most important feature:\n",
        "\n",
        "Train the model with these parameters:\n",
        "n_estimators=10,\n",
        "max_depth=20,\n",
        "random_state=1,\n",
        "n_jobs=-1 (optional)\n",
        "Get the feature importance information from this model\n",
        "What's the most important feature (among these 4)?\n",
        "\n",
        "total_rooms\n",
        "median_income\n",
        "total_bedrooms\n",
        "longitude\n",
        "'''\n",
        "\n",
        "dt = RandomForestRegressor(n_estimators=10, max_depth=20, random_state=1)\n",
        "dt.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "BxUAzzCJ1peA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lst_feat_imp = list(zip(dv.get_feature_names_out(),dt.feature_importances_))\n",
        "lst_feat_imp.sort(key = lambda i:i[1], reverse = True)\n",
        "lst_feat_imp"
      ],
      "metadata": {
        "id": "DisIhVVkdU67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "al7AwV1id2Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dv.get_feature_names_out())"
      ],
      "metadata": {
        "id": "6Ep7FQRohy7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Now let's train an XGBoost model! For this question, we'll tune the eta parameter:\n",
        "  * Install XGBoost\n",
        "  * Create DMatrix for train and validation\n",
        "  * Create a watchlist\n",
        "  * Train a model with these parameters for 100 rounds:\n",
        "'''"
      ],
      "metadata": {
        "id": "eWFKOBw3kYjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install XGBoost"
      ],
      "metadata": {
        "id": "yaAuaKCOixPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "import re"
      ],
      "metadata": {
        "id": "2TRF-gOHfqbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_xgb_output(data_str):\n",
        " # Split the data into lines\n",
        "  lines = data_str.strip().split(\"\\n\")\n",
        "\n",
        "  # Extract data from each line\n",
        "  indices = [int(re.search(r'\\[(\\d+)\\]', line).group(1)) for line in lines]\n",
        "  train_rmse = [float(re.search(r'train-rmse:(\\d+\\.\\d+)', line).group(1)) for line in lines]\n",
        "  val_rmse = [float(re.search(r'val-rmse:(\\d+\\.\\d+)', line).group(1)) for line in lines]\n",
        "\n",
        "  # Create a dataframe from the extracted data\n",
        "  df = pd.DataFrame({\n",
        "      'train_rmse': train_rmse,\n",
        "      'Validation_rmse': val_rmse\n",
        "  })\n",
        "\n",
        "  df['train_rmse'] = df['train_rmse'].round(3)\n",
        "  df['Validation_rmse'] = df['Validation_rmse'].round(3)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "iQCM4d1Gmc1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = dv.get_feature_names_out().tolist()\n",
        "features = list(map(lambda i: str(i).replace(\"<\", \"\").replace(\"=\", \"_\")\n",
        ".replace(\" \", \"_\"), features))\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train, label = y_train, feature_names = features)\n",
        "dval = xgb.DMatrix(X_val, label = y_val, feature_names = features)"
      ],
      "metadata": {
        "id": "dsqBvQ-Vgefx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import redirect_stdout, redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "# Create StringIO objects to capture the output\n",
        "stdout_buffer = StringIO()\n",
        "stderr_buffer = StringIO()\n",
        "\n",
        "xgb_params = {\n",
        "    'eta': 0.3,\n",
        "    'max_depth': 6,\n",
        "    'min_child_weight': 1,\n",
        "\n",
        "    'objective': 'reg:squarederror',\n",
        "    'nthread': 8,\n",
        "\n",
        "    'seed': 1,\n",
        "    'verbosity': 1,\n",
        "}\n",
        "watchlist = [(dtrain,'train'),(dval,'val')]\n",
        "\n",
        "with redirect_stdout(stdout_buffer), redirect_stderr(stderr_buffer):\n",
        "  model = xgb.train(xgb_params, dtrain, num_boost_round=100,\n",
        "                  evals=watchlist)\n",
        "\n",
        "captured_stdout = stdout_buffer.getvalue()\n",
        "captured_stderr = stderr_buffer.getvalue()\n",
        "\n",
        "df_xgb = parse_xgb_output(captured_stdout)\n",
        "df_xgb.sort_values(by=['Validation_rmse'],ascending=True).head()"
      ],
      "metadata": {
        "id": "c8FRzH8Ei0Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create StringIO objects to capture the output\n",
        "stdout_buffer = StringIO()\n",
        "stderr_buffer = StringIO()\n",
        "\n",
        "xgb_params = {\n",
        "    'eta': 0.1,\n",
        "    'max_depth': 6,\n",
        "    'min_child_weight': 1,\n",
        "\n",
        "    'objective': 'reg:squarederror',\n",
        "    'nthread': 8,\n",
        "\n",
        "    'seed': 1,\n",
        "    'verbosity': 1,\n",
        "}\n",
        "\n",
        "with redirect_stdout(stdout_buffer), redirect_stderr(stderr_buffer):\n",
        "  model = xgb.train(xgb_params, dtrain, num_boost_round=100,\n",
        "                  evals=watchlist)\n",
        "\n",
        "captured_stdout = stdout_buffer.getvalue()\n",
        "captured_stderr = stderr_buffer.getvalue()\n",
        "\n",
        "df_xgb = parse_xgb_output(captured_stdout)\n",
        "df_xgb.sort_values(by=['Validation_rmse'],ascending=True).head()\n"
      ],
      "metadata": {
        "id": "KuPNGVWomjhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E0byezwKnEPM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}